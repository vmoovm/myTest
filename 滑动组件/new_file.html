<!DOCTYPE html>
<html>

	<head>
		<meta charset="UTF-8">
		<title></title>
	</head>

	<body>
		<!--<audio src="http://developer.mozilla.org/@api/deki/files/2926/=AudioTest_(1).ogg" id="audo" buffered autoplay controls>
			Your browser does not support the <code>audio</code> element.
		</audio>-->
		<!--<script type="text/javascript">
			var audo = document.getElementById("audo");
			console.log(audo.getAttribute('buffered'))
		</script>-->
		<script type="text/javascript">
			window.AudioContext = window.AudioContext || window.webkitAudioContext || window.mozAudioContext || window.msAudioContext;
			try {
				var audioContext = new window.AudioContext();
			} catch(e) {
				Console.log('!Your browser does not support AudioContext');
			}
			var Visualizer = function() {
				this.file = null, //要处理的文件，后面会讲解如何获取文件
					this.fileName = null, //要处理的文件的名，文件名
					this.audioContext = null, //进行音频处理的上下文，稍后会进行初始化
					this.source = null //保存音频
			};
			Visualizer.prototype = {
				_prepareAPI: function() {
					//统一前缀，方便调用
					window.AudioContext = window.AudioContext || window.webkitAudioContext || window.mozAudioContext || window.msAudioContext;
					//这里顺便也将requestAnimationFrame也打个补丁，后面用来写动画要用
					window.requestAnimationFrame = window.requestAnimationFrame || window.webkitRequestAnimationFrame || window.mozRequestAnimationFrame || window.msRequestAnimationFrame;
					//安全地实例化一个AudioContext并赋值到Visualizer的audioContext属性上，方便后面处理音频使用
					try {
						this.audioContext = new AudioContext();
					} catch(e) {
						console.log('!妳的浏览器不支持AudioContext:(');
						console.log(e);
					}
				},
				_addEventListner: function() {
					var that = this,
						audioInput = document.getElementById('uploadedFile'),
						dropContainer = document.getElementsByTagName("canvas")[0];
					//监听是否有文件被选中
					audioInput.onchange = function() {
						//这里判断一下文件长度可以确定用户是否真的选择了文件，如果点了取消则文件长度为0
						if(audioInput.files.length !== 0) {
							that.file = audioInput.files[0]; //将文件赋值到Visualizer对象的属性上
							that.fileName = that.file.name;
							that._start(); //获取到文件后，开始程序，这个方法会在后面定义并实现
						};
					};
					dropContainer.addEventListener("dragenter", function() {
						that._updateInfo('Drop it on the page', true);
					}, false);
					dropContainer.addEventListener("dragover", function(e) {
						e.stopPropagation();
						e.preventDefault();
						e.dataTransfer.dropEffect = 'copy'; //设置文件放置类型为拷贝
					}, false);
					dropContainer.addEventListener("dragleave", function() {
						that._updateInfo(that.info, false);
					}, false);
					dropContainer.addEventListener("drop", function(e) {
						e.stopPropagation();
						e.preventDefault();
						that.file = e.dataTransfer.files[0]; //获取文件并赋值到Visualizer对象
						that.fileName = that.file.name;
						that._start();
					}, false);
				},
				_start: function() {
					//read and decode the file into audio array buffer
					var that = this, //当前this指代Visualizer对象，赋值给that以以便在其他地方使用
						file = this.file, //从Visualizer对象上获取前面得到的文件
						fr = new FileReader(); //实例化一个FileReader用于读取文件
					fr.onload = function(e) { //文件读取完后调用此函数
						var fileResult = e.target.result; //这是读取成功得到的结果ArrayBuffer数据
						var audioContext = that.audioContext; //从Visualizer得到最开始实例化的AudioContext用来做解码ArrayBuffer
						audioContext.decodeAudioData(fileResult, function(buffer) { //解码成功则调用此函数，参数buffer为解码后得到的结果
							that._visualize(audioContext, buffer); //调用_visualize进行下一步处理，此方法在后面定义并实现
						}, function(e) { //这个是解码失败会调用的函数
							console.log("!哎玛，文件解码失败:(");
						});
					};
					//将上一步获取的文件传递给FileReader从而将其读取为ArrayBuffer格式
					fr.readAsArrayBuffer(file);
				},
				_visualize: function(audioContext, buffer) {
/*					var audioBufferSouceNode = audioContext.createBufferSource();
					audioBufferSouceNode.buffer = buffer;*/
					var audioBufferSouceNode = audioContext.createBufferSource();
					audioBufferSouceNode.connect(audioContext.destination);
					audioBufferSouceNode.buffer = buffer;
					audioBufferSouceNode.start(0);
				},
				_visualize: function(audioContext, buffer) {
					var audioBufferSouceNode = audioContext.createBufferSource(),
						analyser = audioContext.createAnalyser();
					//将source与分析器连接
					audioBufferSouceNode.connect(analyser);
					//将分析器与destination连接，这样才能形成到达扬声器的通路
					analyser.connect(audioContext.destination);
					//将上一步解码得到的buffer数据赋值给source
					audioBufferSouceNode.buffer = buffer;
					//播放
					audioBufferSouceNode.start(0);
					//音乐响起后，把analyser传递到另一个方法开始绘制频谱图了，因为绘图需要的信息要从analyser里面获取
					this._drawSpectrum(analyser);
				},
				_drawSpectrum: function(analyser) {
					var canvas = document.getElementById('canvas'),
						cwidth = canvas.width,
						cheight = canvas.height - 2,
						meterWidth = 10, //频谱条宽度
						gap = 2, //频谱条间距
						capHeight = 2,
						capStyle = '#fff',
						meterNum = 800 / (10 + 2), //频谱条数量
						capYPositionArray = []; //将上一画面各帽头的位置保存到这个数组
					ctx = canvas.getContext('2d'),
						gradient = ctx.createLinearGradient(0, 0, 0, 300);
					gradient.addColorStop(1, '#0f0');
					gradient.addColorStop(0.5, '#ff0');
					gradient.addColorStop(0, '#f00');
					var drawMeter = function() {
						var array = new Uint8Array(analyser.frequencyBinCount);
						analyser.getByteFrequencyData(array);
						var step = Math.round(array.length / meterNum); //计算采样步长
						ctx.clearRect(0, 0, cwidth, cheight);
						for(var i = 0; i < meterNum; i++) {
							var value = array[i * step]; //获取当前能量值
							if(capYPositionArray.length < Math.round(meterNum)) {
								capYPositionArray.push(value); //初始化保存帽头位置的数组，将第一个画面的数据压入其中
							};
							ctx.fillStyle = capStyle;
							//开始绘制帽头
							if(value < capYPositionArray[i]) { //如果当前值小于之前值
								ctx.fillRect(i * 12, cheight - (--capYPositionArray[i]), meterWidth, capHeight); //则使用前一次保存的值来绘制帽头
							} else {
								ctx.fillRect(i * 12, cheight - value, meterWidth, capHeight); //否则使用当前值直接绘制
								capYPositionArray[i] = value;
							};
							//开始绘制频谱条
							ctx.fillStyle = gradient;
							ctx.fillRect(i * 12, cheight - value + capHeight, meterWidth, cheight);
						}
						requestAnimationFrame(drawMeter);
					}
					requestAnimationFrame(drawMeter);
				}
			}
		</script>
		<script type="text/javascript">
			loadSound("music/Neptune Illusion Dennis Kuo .mp3"); //调用
			// 定义加载音频文件的函数
			function loadSound(url) {
				var request = new XMLHttpRequest(); //建立一个请求
				request.open('GET', url, true); //配置好请求类型，文件路径等
				request.responseType = 'arraybuffer'; //配置数据返回类型
				// 一旦获取完成，对音频进行进一步操作，比如解码
				request.onload = function() {
					var arraybuffer = request.response;
				}
				request.send();
			}
		</script>
		<label for="uploadedFile">Drag&drop or select a file to play:</label>
		<input type="file" id="uploadedFile"></input>
		<div id="wrapper">
			<div id="fileWrapper" class="file_wrapper">
				<div id="info">
					HTML5 Audio API showcase | An Audio Viusalizer
				</div>
				<label for="uploadedFile">Drag&drop or select a file to play:</label>
				<input type="file" id="uploadedFile"></input>
			</div>
			<div id="visualizer_wrapper">
				<canvas id='canvas' width="800" height="350"></canvas>
			</div>
		</div>
		<style type="text/css">
			#fileWrapper {
				transition: all 0.5s ease;
			}
			
			#fileWrapper: hover {
				opacity: 1!important;
			}
			
			#visualizer_wrapper {
				text-align: center;
			}
		</style>
		<script type="text/javascript">
       /*
 *An audio spectrum visualizer built with HTML5 Audio API
 *Author:Wayou
 *License:feel free to use but keep this info please!
 *Feb 15 2014
 *Need support you can :
 *view the project page:https://github.com/Wayou/HTML5_Audio_Visualizer/
 *view online demo:http://wayouliu.duapp.com/mess/audio_visualizer.html
 *or contact me:liuwayong@gmail.com
 */
window.onload = function() {
    new Visualizer().ini();
};
var Visualizer = function() {
    this.file = null, //the current file
    this.fileName = null, //the current file name
    this.audioContext = null,
    this.source = null, //the audio source
    this.info = document.getElementById('info').innerHTML, //this used to upgrade the UI information
    this.infoUpdateId = null, //to sotore the setTimeout ID and clear the interval
    this.forceStop = false
};
Visualizer.prototype = {
    ini: function() {
        this._prepareAPI();
        this._addEventListner();
    },
    _prepareAPI: function() {
        //fix browser vender for AudioContext and requestAnimationFrame
        window.AudioContext = window.AudioContext || window.webkitAudioContext || window.mozAudioContext || window.msAudioContext;
        window.requestAnimationFrame = window.requestAnimationFrame || window.webkitRequestAnimationFrame || window.mozRequestAnimationFrame || window.msRequestAnimationFrame;
        try {
            this.audioContext = new AudioContext();
        } catch (e) {
            that._updateInfo('!Your browser does not support AudioContext', false);
            console.log(e);
        }
    },
    _addEventListner: function() {
        var that = this,
            audioInput = document.getElementById('uploadedFile'),
            dropContainer = document.getElementsByTagName("canvas")[0];
        //listen the file upload
        audioInput.onchange = function() {
            //the if statement fixes the file selction cancle, because the onchange will trigger even the file selection been canceled
            if (audioInput.files.length !== 0) {
                //only process the first file
                that.file = audioInput.files[0];
                that.fileName = that.file.name;
                //once the file is ready,start the visualizer
                that._start();
                that._updateInfo('Uploading', true);
            };
        };
        //listen the drag & drop
        dropContainer.addEventListener("dragenter", function() {
            that._updateInfo('Drop it on the page', true);
        }, false);
        dropContainer.addEventListener("dragover", function(e) {
            e.stopPropagation();
            e.preventDefault();
            //set the drop mode
            e.dataTransfer.dropEffect = 'copy';
        }, false);
        dropContainer.addEventListener("dragleave", function() {
            that._updateInfo(that.info, false);
        }, false);
        dropContainer.addEventListener("drop", function(e) {
            e.stopPropagation();
            e.preventDefault();
            that._updateInfo('Uploading', true);
            //get the dropped file
            that.file = e.dataTransfer.files[0];
            that.fileName = that.file.name;
            //once the file is ready,start the visualizer
            that._start();
        }, false);
    },
    _start: function() {
        //read and decode the file into audio array buffer 
        var that = this,
            file = this.file,
            fr = new FileReader();
        fr.onload = function(e) {
            var fileResult = e.target.result;
            var audioContext = that.audioContext;
            if (audioContext === null) {
                return;
            };
            that._updateInfo('Decoding the audio', true);
            audioContext.decodeAudioData(fileResult, function(buffer) {
                that._updateInfo('Decode succussfully,start the visualizer', true);
                that._visualize(audioContext, buffer);
            }, function(e) {
                that._updateInfo('!Fail to decode the file', false);
                console.log(e);
            });
        };
        fr.onerror = function(e) {
            that._updateInfo('!Fail to read the file', false);
            console.log(e);
        };
        //assign the file to the reader
        this._updateInfo('Starting read the file', true);
        fr.readAsArrayBuffer(file);
    },
    _visualize: function(audioContext, buffer) {
        var audioBufferSouceNode = audioContext.createBufferSource(),
            analyser = audioContext.createAnalyser(),
            that = this;
        //connect the source to the analyser
        audioBufferSouceNode.connect(analyser);
        //connect the analyser to the destination(the speaker), or we won't hear the sound
        analyser.connect(audioContext.destination);
        //then assign the buffer to the buffer source node
        audioBufferSouceNode.buffer = buffer;
        //play the source
        if (!audioBufferSouceNode.start) {
            audioBufferSouceNode.start = audioBufferSouceNode.noteOn //in old browsers use noteOn method
            audioBufferSouceNode.stop = audioBufferSouceNode.noteOff //in old browsers use noteOn method
        };
        audioBufferSouceNode.start(0);
        audioBufferSouceNode.onended = function() {
            that._audioEnd(that);
        };
        //stop the previous sound if any
        if (this.source !== null) {
            this.forceStop = true;
            this.source.stop(0);
        }
        this.source = audioBufferSouceNode;
        this._updateInfo('Playing ' + this.fileName, false);
        this.info = 'Playing ' + this.fileName;
        document.getElementById('fileWrapper').style.opacity = 0.2;
        this._drawSpectrum(analyser);
    },
    _drawSpectrum: function(analyser) {
        var canvas = document.getElementById('canvas'),
            cwidth = canvas.width,
            cheight = canvas.height - 2,
            meterWidth = 10, //width of the meters in the spectrum
            gap = 2, //gap between meters
            capHeight = 2,
            capStyle = '#fff',
            meterNum = 800 / (10 + 2), //count of the meters
            capYPositionArray = []; ////store the vertical position of hte caps for the preivous frame
        ctx = canvas.getContext('2d'),
        gradient = ctx.createLinearGradient(0, 0, 0, 300);
        gradient.addColorStop(1, '#0f0');
        gradient.addColorStop(0.5, '#ff0');
        gradient.addColorStop(0, '#f00');
       var drawMeter = function() {
            var array = new Uint8Array(analyser.frequencyBinCount);
            analyser.getByteFrequencyData(array);
            ctx.clearRect(0, 0, cwidth, cheight);
            for (var i = 0; i < array.length; i++) {
                var value = array[i];
                if (capYPositionArray.length < Math.round(meterNum)) {
                    capYPositionArray.push(value);
                };
                ctx.fillStyle = gradient; //set the filllStyle to gradient for a better look
                ctx.fillRect(i /*meterWidth+gap*/ , cheight - value  /*2 is the gap between meter and cap*/ , 1, cheight); //the meter
            }
            requestAnimationFrame(drawMeter);
        }
        requestAnimationFrame(drawMeter);
    },
    _audioEnd: function(instance) {
        if (this.forceStop) {
            this.forceStop=false;
            return;
        };
        console.log('audio ended');
        var canvas = document.getElementById('canvas'),
            cwidth = canvas.width,
            cheight = canvas.height,
            ctx = canvas.getContext('2d'),
            text = 'HTML5 Audio API showcase | An Audio Viusalizer';
        ctx.clearRect(0, 0, cwidth, cheight);
        document.getElementById('fileWrapper').style.opacity = 1;
        document.getElementById('info').innerHTML = text;
        instance.info = text;
        document.getElementById('uploadedFile').value = '';
    },
    _updateInfo: function(text, processing) {
        var infoBar = document.getElementById('info'),
            dots = '...',
            i = 0,
            that = this;
        infoBar.innerHTML = text + dots.substring(0, i++);
        if (this.infoUpdateId !== null) {
            clearTimeout(this.infoUpdateId);
        };
        if (processing) {
            //animate dots at the end of the info text
            var animateDot = function() {
                if (i > 3) {
                    i = 0
                };
                infoBar.innerHTML = text + dots.substring(0, i++);
                that.infoUpdateId = setTimeout(animateDot, 250);
            }
            this.infoUpdateId = setTimeout(animateDot, 250);
        };
    }
}
        </script>
		<script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-46794744-5', 'duapp.com');
        ga('send', 'pageview');
        </script>
	</body>

</html>